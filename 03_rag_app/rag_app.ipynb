{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39108bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15be01",
   "metadata": {},
   "source": [
    "# 构建一个检索增强生成 (RAG) 应用\n",
    "RAG是一种通过额外数据增强大型语言模型知识的技术。\n",
    "\n",
    "大型语言模型可以推理广泛的主题，但它们的知识仅限于训练时使用的特定时间点的公共数据。如果您想构建能够推理私有数据或在模型截止日期后引入的数据的AI应用程序，您需要用模型所需的特定信息来增强模型的知识。将适当的信息引入并插入到模型提示中的过程称为检索增强生成（RAG）。\n",
    "\n",
    "LangChain有许多组件旨在帮助构建问答应用程序，以及更一般的RAG应用程序。\n",
    "\n",
    "注意：在这里我们专注于非结构化数据的问答。如果您对结构化数据的RAG感兴趣，请查看我们关于SQL数据的问答的教程。\n",
    "\n",
    "## 概念\n",
    "一个典型的RAG应用程序有两个主要组件：\n",
    "\n",
    "索引：一个从源数据中摄取数据并进行索引的管道。这通常在离线进行。\n",
    "\n",
    "检索和生成：实际的RAG链，在运行时接收用户查询并从索引中检索相关数据，然后将其传递给模型。\n",
    "\n",
    "从原始数据到答案的最常见完整序列如下：\n",
    "\n",
    "## 索引\n",
    "加载: 首先我们需要加载我们的数据。这是通过 文档加载器 完成的。\n",
    "分割: 文本分割器 将大型 文档 分割成较小的块。这对于索引数据和将其传递给模型都很有用，因为大型块更难搜索，并且无法适应模型的有限上下文窗口。\n",
    "存储: 我们需要一个地方来存储和索引我们的分割，以便后续可以进行搜索。这通常使用 向量存储 和 嵌入模型 来完成。\n",
    "\n",
    "## 检索与生成\n",
    "检索: 根据用户输入，从存储中使用 检索器 检索相关的分割。\n",
    "生成: 聊天模型 / 大型语言模型 使用包含问题和检索数据的提示生成答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6902aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: True\n",
      "LANGCHAIN_API_KEY: True\n"
     ]
    }
   ],
   "source": [
    "# 从电脑环境变量中导入\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", OPENAI_API_KEY is not None)\n",
    "print(\"LANGCHAIN_API_KEY:\", LANGCHAIN_API_KEY is not None)\n",
    "\n",
    "# 设置环境变量以启用LangChain的跟踪功能\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "# 设置项目名称以便在LangChain跟踪界面中区分不同项目\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"llm-langchain-learn\"\n",
    "\n",
    "# 兼容 DashScope：将同一个 Key 同步给 DASHSCOPE_API_KEY（若你单独配置了，可删掉此行）\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3378318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"qwen-plus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da79b359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dashscope'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     23\u001b[0m splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs)\n\u001b[0;32m     24\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m     25\u001b[0m     documents\u001b[38;5;241m=\u001b[39msplits,\n\u001b[1;32m---> 26\u001b[0m     embedding\u001b[38;5;241m=\u001b[39m\u001b[43mDashScopeEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 从环境变量 DASHSCOPE_API_KEY 读取 Key\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Retrieve and generate using the relevant snippets of the blog.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\software\\ProgramTool\\Miniconda3\\envs\\llm-env\\lib\\site-packages\\langchain_community\\embeddings\\dashscope.py:128\u001b[0m, in \u001b[0;36mDashScopeEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@model_validator\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdashscope\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate that api key and python package exists in environment.\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdashscope_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_from_dict_or_env(\n\u001b[0;32m    132\u001b[0m         values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdashscope_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDASHSCOPE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dashscope'"
     ]
    }
   ],
   "source": [
    "# 我们将构建一个应用程序，回答有关网站内容的问题。\n",
    "import bs4\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=DashScopeEmbeddings()  # 从环境变量 DASHSCOPE_API_KEY 读取 Key\n",
    ")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 直接在本地构造 RAG 提示，避免依赖 langchain.hub\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant. Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
