{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39108bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15be01",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LangChain æ„å»º RAG ä»£ç†\n",
    "LLM æœ€å¼ºå¤§çš„åº”ç”¨ä¹‹ä¸€æ˜¯å¤æ‚çš„é—®ç­” (Q&A) èŠå¤©æœºå™¨äººã€‚è¿™äº›åº”ç”¨ç¨‹åºå¯ä»¥å›ç­”å…³äºç‰¹å®šæ¥æºä¿¡æ¯çš„é—®é¢˜ã€‚è¿™äº›åº”ç”¨ç¨‹åºä½¿ç”¨ä¸€ç§ç§°ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval Augmented Generation)ï¼Œæˆ– RAG çš„æŠ€æœ¯ã€‚\n",
    "æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªåŸºäºéç»“æ„åŒ–æ–‡æœ¬æ•°æ®æºçš„ç®€å•é—®ç­”åº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬å°†æ¼”ç¤ºï¼š\n",
    "ä¸€ä¸ª RAG ä»£ç†ï¼Œå®ƒä½¿ç”¨ç®€å•çš„å·¥å…·æ‰§è¡Œæœç´¢ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€šç”¨å®ç°ã€‚\n",
    "ä¸€ä¸ªä¸¤æ­¥ RAG é“¾ï¼Œæ¯ä¸ªæŸ¥è¯¢åªä½¿ç”¨ä¸€æ¬¡ LLM è°ƒç”¨ã€‚è¿™å¯¹äºç®€å•æŸ¥è¯¢æ¥è¯´æ˜¯ä¸€ç§å¿«é€Ÿæœ‰æ•ˆçš„æ–¹æ³•ã€‚\n",
    "\n",
    "## æ¦‚å¿µ\n",
    "ä¸€ä¸ªå…¸å‹çš„RAGåº”ç”¨ç¨‹åºæœ‰ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼š\n",
    "\n",
    "ç´¢å¼•ï¼šä¸€ä¸ªä»æºæ•°æ®ä¸­æ‘„å–æ•°æ®å¹¶è¿›è¡Œç´¢å¼•çš„ç®¡é“ã€‚è¿™é€šå¸¸åœ¨ç¦»çº¿è¿›è¡Œã€‚\n",
    "\n",
    "æ£€ç´¢å’Œç”Ÿæˆï¼šå®é™…çš„RAGé“¾ï¼Œåœ¨è¿è¡Œæ—¶æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢å¹¶ä»ç´¢å¼•ä¸­æ£€ç´¢ç›¸å…³æ•°æ®ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™æ¨¡å‹ã€‚\n",
    "\n",
    "ä»åŸå§‹æ•°æ®åˆ°ç­”æ¡ˆçš„æœ€å¸¸è§å®Œæ•´åºåˆ—å¦‚ä¸‹ï¼š\n",
    "\n",
    "## ç´¢å¼•\n",
    "åŠ è½½: é¦–å…ˆæˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬çš„æ•°æ®ã€‚è¿™æ˜¯é€šè¿‡ æ–‡æ¡£åŠ è½½å™¨ å®Œæˆçš„ã€‚\n",
    "åˆ†å‰²: æ–‡æœ¬åˆ†å‰²å™¨ å°†å¤§å‹ æ–‡æ¡£ åˆ†å‰²æˆè¾ƒå°çš„å—ã€‚è¿™å¯¹äºç´¢å¼•æ•°æ®å’Œå°†å…¶ä¼ é€’ç»™æ¨¡å‹éƒ½å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå¤§å‹å—æ›´éš¾æœç´¢ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”æ¨¡å‹çš„æœ‰é™ä¸Šä¸‹æ–‡çª—å£ã€‚\n",
    "å­˜å‚¨: æˆ‘ä»¬éœ€è¦ä¸€ä¸ªåœ°æ–¹æ¥å­˜å‚¨å’Œç´¢å¼•æˆ‘ä»¬çš„åˆ†å‰²ï¼Œä»¥ä¾¿åç»­å¯ä»¥è¿›è¡Œæœç´¢ã€‚è¿™é€šå¸¸ä½¿ç”¨ å‘é‡å­˜å‚¨ å’Œ åµŒå…¥æ¨¡å‹ æ¥å®Œæˆã€‚\n",
    "\n",
    "## æ£€ç´¢ä¸ç”Ÿæˆ\n",
    "æ£€ç´¢: æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œä»å­˜å‚¨ä¸­ä½¿ç”¨ æ£€ç´¢å™¨ æ£€ç´¢ç›¸å…³çš„åˆ†å‰²ã€‚\n",
    "ç”Ÿæˆ: èŠå¤©æ¨¡å‹ / å¤§å‹è¯­è¨€æ¨¡å‹ ä½¿ç”¨åŒ…å«é—®é¢˜å’Œæ£€ç´¢æ•°æ®çš„æç¤ºç”Ÿæˆç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6902aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: True\n",
      "LANGCHAIN_API_KEY: True\n",
      "sk-85b55aebba56452b8b77e2dbc5006486\n"
     ]
    }
   ],
   "source": [
    "# ä»ç”µè„‘ç¯å¢ƒå˜é‡ä¸­å¯¼å…¥\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", OPENAI_API_KEY is not None)\n",
    "print(\"LANGCHAIN_API_KEY:\", LANGCHAIN_API_KEY is not None)\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨LangChainçš„è·Ÿè¸ªåŠŸèƒ½\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "# è®¾ç½®é¡¹ç›®åç§°ä»¥ä¾¿åœ¨LangChainè·Ÿè¸ªç•Œé¢ä¸­åŒºåˆ†ä¸åŒé¡¹ç›®\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"llm-langchain-learn\"\n",
    "\n",
    "# # å…¼å®¹ DashScopeï¼šå°†åŒä¸€ä¸ª Key åŒæ­¥ç»™ DASHSCOPE_API_KEYï¼ˆè‹¥ä½ å•ç‹¬é…ç½®äº†ï¼Œå¯åˆ æ‰æ­¤è¡Œï¼‰\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"qwen-plus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79b359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps or subtasks. This approach allows for better planning and execution, especially in complicated problem-solving scenarios. In the context of AI and language models, task decomposition can be achieved through methods like Chain of Thought (CoT), where the model is prompted to \"think step by step,\" decomposing the task into simpler reasoning steps. Tree of Thoughts extends this by exploring multiple reasoning paths simultaneously, forming a tree structure of possible thoughts and solutions. Decomposition can be facilitated by prompting the LLM directly, using task-specific instructions, incorporating human input, or even integrating external planners like in the LLM+P framework, which uses PDDL for formal planning. The goal is to enhance performance on complex tasks by structuring them into sequential or hierarchical subgoals.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå›ç­”æœ‰å…³ç½‘ç«™å†…å®¹çš„é—®é¢˜ã€‚\n",
    "import bs4\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=DashScopeEmbeddings()  # ä»ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY è¯»å– Key\n",
    ")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# ç›´æ¥åœ¨æœ¬åœ°æ„é€  RAG æç¤ºï¼Œé¿å…ä¾èµ– langchain.hub\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant. Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b5920",
   "metadata": {},
   "source": [
    "# ç»„ä»¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc81098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! However, there might be a small mix-up. You said \"Hello, world!\", which is typically something a program or a new AI might say when starting up. But I\\'m here and ready to helpâ€”how can I assist you today? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 12, 'total_tokens': 65, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-01f2b63f-3d36-989f-ab93-ac6f8c9a1aea', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bbbcc-da21-7561-a3fe-bc920ac57f42-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 53, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "# é€‰æ‹©æ¨¡å‹\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"qwen-plus\"\n",
    ")\n",
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285349f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€ä¸ªå‘é‡æ¨¡å‹,ä½¿ç”¨é€šä¹‰åƒé—®çš„ text-embedding-v3 æ¨¡å‹ï¼ˆDashScopeï¼‰\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "# ä½¿ç”¨é€šä¹‰åƒé—®çš„ text-embedding-v3 æ¨¡å‹ï¼ˆDashScopeï¼‰\n",
    "# è¿™é‡Œå’Œå®˜ç½‘çš„ç¤ºä¾‹ç¨æœ‰ä¸åŒ\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\",\n",
    "    dashscope_api_key=OPENAI_API_KEY  # ä½¿ç”¨ä½ çš„é˜¿é‡Œäº‘ API Key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77ad79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€ä¸ªå‘é‡å­˜å‚¨,langchain_core.vectorstoresæ˜¯\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcf9a9",
   "metadata": {},
   "source": [
    "# ç´¢å¼•\n",
    "ç´¢å¼•é€šå¸¸å¦‚ä¸‹ï¼š\n",
    "åŠ è½½ï¼šé¦–å…ˆæˆ‘ä»¬éœ€è¦åŠ è½½æ•°æ®ã€‚è¿™æ˜¯ç”¨æ–‡æ¡£åŠ è½½å™¨.\n",
    "åˆ†è£‚ï¼šæ–‡æœ¬åˆ†é…å™¨æŠŠå¤§å—æ‹†æˆæ›´å°çš„å—ã€‚è¿™å¯¹æ•°æ®ç´¢å¼•å’Œæ•°æ®ä¼ é€’åˆ°æ¨¡å‹ä¸­éƒ½å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå¤§å—å—æ›´éš¾æœç´¢ï¼Œä¸”æ— æ³•æ”¾å…¥æ¨¡å‹æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€‚Documents\n",
    "å­˜å‚¨ï¼šæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåœ°æ–¹æ¥å­˜å‚¨å’Œç´¢å¼•æ‹†åˆ†ï¼Œä»¥ä¾¿ä»¥åå¯ä»¥æœç´¢ã€‚è¿™é€šå¸¸é€šè¿‡VectorStoreä»¥åŠåµŒå…¥æ¨¡ç‰¹ã€‚\n",
    "## åŠ è½½æ–‡æ¡£\n",
    "æˆ‘ä»¬éœ€è¦å…ˆåŠ è½½åšå®¢å†…å®¹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–‡æ¡£åŠ è½½å™¨å¯¹äºæ­¤ï¼Œè¿™äº›å¯¹è±¡æ˜¯ä»æºåŠ è½½æ•°æ®å¹¶è¿”å›åˆ—è¡¨çš„å¯¹è±¡æ–‡ä»¶ç‰©å“ã€‚\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨WebBaseLoaderï¼Œç”¨äºä»ç½‘é¡µURLåŠ è½½HTMLï¼Œå¹¶å°†å…¶è§£ææˆæ–‡æœ¬ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘è§£æå™¨è¾“å…¥å‚æ•°ï¼ˆå‚è§ï¼‰æ¥è‡ªå®šä¹‰ HTML -> æ–‡æœ¬è§£æurllib BeautifulSoup BeautifulSoupbs_kwargs BeautifulSoupæ–‡æ¡£).åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªæœ‰ç±»ä¸ºâ€œpost-contentâ€ã€â€œpost-titleâ€æˆ–â€œpost-headerâ€çš„HTMLæ ‡ç­¾ç›¸å…³ï¼Œæˆ‘ä»¬å°†ç§»é™¤æ‰€æœ‰å…¶ä»–æ ‡ç­¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5be6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# ä»…ä¿ç•™å®Œæ•´çš„ HTML ä¸­çš„æ ‡é¢˜ã€å‰¯æ ‡é¢˜å’Œå†…å®¹éƒ¨åˆ†ã€‚\n",
    "bs4_strainer = bs4.SoupStrainer(\n",
    "    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    ")\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    bs_kwargs=dict(parse_only=bs4_strainer) # é‡‡ç”¨è¿‡æ»¤å™¨\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc02027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bc87f",
   "metadata": {},
   "source": [
    "## æ–‡ä»¶æ‹†åˆ†\n",
    "æˆ‘ä»¬çš„åŠ è½½æ–‡æ¡£è¶…è¿‡42kå­—ç¬¦ï¼Œå¤ªé•¿ï¼Œæ— æ³•æ”¾å…¥è®¸å¤šæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ã€‚å³ä½¿æ˜¯é‚£äº›èƒ½åœ¨ä¸Šä¸‹æ–‡çª—å£ä¸­å®¹çº³å®Œæ•´å¸–å­çš„æ¨¡å‹ï¼Œæ¨¡å‹åœ¨éå¸¸é•¿çš„è¾“å…¥ä¸­ä¹Ÿå¯èƒ½éš¾ä»¥æ‰¾åˆ°ä¿¡æ¯ã€‚\n",
    "ä¸ºäº†å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†åˆ†å¼€Documentåˆ†å—ç”¨äºåµŒå…¥å’Œå‘é‡å­˜å‚¨ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬åœ¨è¿è¡Œæ—¶åªæ£€ç´¢åšå®¢æ–‡ç« ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ã€‚\n",
    "æ¯”å¦‚è¯´è¯­ä¹‰æœç´¢æ•™ç¨‹æˆ‘ä»¬ä½¿ç”¨ ä¸€ä¸ª ï¼Œå®ƒä¼šé€’å½’åœ°ä½¿ç”¨å¸¸ç”¨åˆ†éš”ç¬¦ï¼ˆå¦‚æ–°è¡Œï¼‰æ‹†åˆ†æ–‡æ¡£ï¼Œç›´åˆ°æ¯ä¸ªåŒºå—å¤§å°åˆé€‚ã€‚è¿™æ˜¯é€šç”¨æ–‡æœ¬ç”¨ä¾‹ä¸­æ¨èçš„æ–‡æœ¬æ‹†åˆ†å™¨`RecursiveCharacterTextSplitter`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4bac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e2c7e",
   "metadata": {},
   "source": [
    "## å­˜å‚¨æ–‡ä»¶\n",
    "ç°åœ¨æˆ‘ä»¬éœ€è¦ç´¢å¼•63ä¸ªæ–‡æœ¬å—ï¼Œä»¥ä¾¿åœ¨è¿è¡Œæ—¶æœç´¢å®ƒä»¬ã€‚ç»§è¯­ä¹‰æœç´¢æ•™ç¨‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯åµŒå…¥æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹ä¼šæ‹†åˆ†å¹¶æ’å…¥è¿™äº›åµŒå…¥åˆ°å‘é‡å­˜å‚¨.ç»™å®šè¾“å…¥æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡æœç´¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc72307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c12d8d20-e44f-4c17-b74a-bcfb76f73691', '6e60f09a-581f-43f8-9b72-d9c12ccd4867', '5b098148-76d2-4ce9-84f6-7bdaaa1f8fb5']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ae59b",
   "metadata": {},
   "source": [
    "# æ£€ç´¢ä¸ç”Ÿæˆ\n",
    "RAGåº”ç”¨é€šå¸¸å¦‚ä¸‹å·¥ä½œï¼š\n",
    "æ£€ç´¢ï¼šç»™å®šç”¨æˆ·è¾“å…¥ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼ä»å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³æ‹†åˆ†.\n",
    "ç”Ÿæˆï¼šæ¨¡å‹ä½¿ç”¨åŒ…å«é—®é¢˜å’Œæ£€ç´¢æ•°æ®çš„æç¤ºç”Ÿæˆç­”æ¡ˆ\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬ç¼–å†™å®é™…çš„åº”ç”¨ç¨‹åºé€»è¾‘ã€‚æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åºï¼Œå®ƒæ¥å—ç”¨æˆ·é—®é¢˜ï¼Œæœç´¢ä¸è¯¥é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ï¼Œå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œåˆå§‹é—®é¢˜ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶è¿”å›ç­”æ¡ˆã€‚\n",
    "æˆ‘ä»¬å°†æ¼”ç¤ºï¼š\n",
    "\n",
    "ä¸€ä¸ª RAG ä»£ç†ï¼Œå®ƒä½¿ç”¨ç®€å•çš„å·¥å…·æ‰§è¡Œæœç´¢ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€šç”¨å®ç°ã€‚\n",
    "ä¸€ä¸ªä¸¤æ­¥ RAG é“¾ï¼Œæ¯ä¸ªæŸ¥è¯¢åªä½¿ç”¨ä¸€æ¬¡ LLM è°ƒç”¨ã€‚è¿™å¯¹äºç®€å•æŸ¥è¯¢æ¥è¯´æ˜¯ä¸€ç§å¿«é€Ÿæœ‰æ•ˆçš„æ–¹æ³•ã€‚\n",
    "\n",
    "## RAGä»£ç†\n",
    "RAG åº”ç”¨ç¨‹åºçš„ä¸€ç§å½¢å¼æ˜¯ä½œä¸ºä¸€ä¸ªç®€å•çš„ ä»£ç†ï¼Œå¸¦æœ‰ä¸€ä¸ªæ£€ç´¢ä¿¡æ¯çš„å·¥å…·ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å®ç°ä¸€ä¸ªåŒ…è£…æˆ‘ä»¬å‘é‡å­˜å‚¨çš„ å·¥å…· æ¥ç»„è£…ä¸€ä¸ªæœ€å°çš„ RAG ä»£ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "# å®šä¹‰ä¸€ä¸ªå·¥å…·ï¼Œä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³å†…å®¹\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_content(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents from the vector store and format them.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent:{doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042a7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºä»£ç†\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "tools = [retrieve_content]\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model=model, tools=tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6553da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "ä»»åŠ¡åˆ†è§£çš„æ ‡å‡†æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "\n",
      "ä¸€æ—¦å¾—å‡ºç­”æ¡ˆï¼Œè¯·æŸ¥æ‰¾è¯¥æ–¹æ³•çš„å¸¸è§æ‰©å±•å½¢å¼ã€‚\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_content (call_7cf528f4b8eb4131a0e96c)\n",
      " Call ID: call_7cf528f4b8eb4131a0e96c\n",
      "  Args:\n",
      "    query: ä»»åŠ¡åˆ†è§£çš„æ ‡å‡†æ–¹æ³•\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_content (call_7cf528f4b8eb4131a0e96c)\n",
      " Call ID: call_7cf528f4b8eb4131a0e96c\n",
      "  Args:\n",
      "    query: ä»»åŠ¡åˆ†è§£çš„æ ‡å‡†æ–¹æ³•\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_content\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content:Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content:Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29546}\n",
      "Content:Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_content\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content:Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content:Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29546}\n",
      "Content:Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_content (call_9c772b4946194138b1652e)\n",
      " Call ID: call_9c772b4946194138b1652e\n",
      "  Args:\n",
      "    query: ä»»åŠ¡åˆ†è§£æ–¹æ³•çš„å¸¸è§æ‰©å±•å½¢å¼\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_content (call_9c772b4946194138b1652e)\n",
      " Call ID: call_9c772b4946194138b1652e\n",
      "  Args:\n",
      "    query: ä»»åŠ¡åˆ†è§£æ–¹æ³•çš„å¸¸è§æ‰©å±•å½¢å¼\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_content\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content:Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content:Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7445}\n",
      "Content:After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_content\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content:Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content:Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7445}\n",
      "Content:After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä»»åŠ¡åˆ†è§£çš„æ ‡å‡†æ–¹æ³•æ˜¯**é“¾å¼æ€ç»´ï¼ˆChain of Thought, CoTï¼‰**ã€‚è¯¥æ–¹æ³•é€šè¿‡æç¤ºæ¨¡å‹â€œé€æ­¥æ€è€ƒâ€ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ›´å°ã€æ›´ç®€å•çš„æ­¥éª¤ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚CoT å°†å¤§ä»»åŠ¡è½¬åŒ–ä¸ºå¤šä¸ªå¯ç®¡ç†çš„å°ä»»åŠ¡ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚\n",
      "\n",
      "è¯¥æ–¹æ³•çš„å¸¸è§æ‰©å±•å½¢å¼æ˜¯**æ€ç»´æ ‘ï¼ˆTree of Thoughts, ToTï¼‰**ã€‚ToT åœ¨ CoT çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•ï¼Œå…è®¸åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­æ¢ç´¢å¤šç§å¯èƒ½çš„æ€è·¯ï¼Œå½¢æˆä¸€ä¸ªæ ‘çŠ¶ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼š\n",
      "\n",
      "1. å°†é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªæ€ç»´æ­¥éª¤ï¼›\n",
      "2. åœ¨æ¯ä¸€æ­¥ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„â€œæ€ç»´â€ï¼ˆå³ä¸­é—´æ¨ç†è·¯å¾„ï¼‰ï¼Œæ„å»ºå‡ºä¸€æ£µæ€ç»´æ ‘ï¼›\n",
      "3. ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰æˆ–æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰è¿›è¡Œæœç´¢ï¼›\n",
      "4. æ¯ä¸ªçŠ¶æ€é€šè¿‡åˆ†ç±»å™¨ï¼ˆç”±æç¤ºå®ç°ï¼‰æˆ–å¤šæ•°æŠ•ç¥¨è¿›è¡Œè¯„ä¼°ï¼Œä»¥å†³å®šæœ€ä½³è·¯å¾„ã€‚\n",
      "\n",
      "è¿™ç§æ–¹æ³•å¢å¼ºäº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šä¸ªæ½œåœ¨è§£å†³æ–¹æ¡ˆä¸­è¿›è¡Œè§„åˆ’å’Œé€‰æ‹©ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦è¯•é”™å’Œå‰ç»è§„åˆ’çš„å¤æ‚ä»»åŠ¡ã€‚\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä»»åŠ¡åˆ†è§£çš„æ ‡å‡†æ–¹æ³•æ˜¯**é“¾å¼æ€ç»´ï¼ˆChain of Thought, CoTï¼‰**ã€‚è¯¥æ–¹æ³•é€šè¿‡æç¤ºæ¨¡å‹â€œé€æ­¥æ€è€ƒâ€ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ›´å°ã€æ›´ç®€å•çš„æ­¥éª¤ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚CoT å°†å¤§ä»»åŠ¡è½¬åŒ–ä¸ºå¤šä¸ªå¯ç®¡ç†çš„å°ä»»åŠ¡ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚\n",
      "\n",
      "è¯¥æ–¹æ³•çš„å¸¸è§æ‰©å±•å½¢å¼æ˜¯**æ€ç»´æ ‘ï¼ˆTree of Thoughts, ToTï¼‰**ã€‚ToT åœ¨ CoT çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•ï¼Œå…è®¸åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­æ¢ç´¢å¤šç§å¯èƒ½çš„æ€è·¯ï¼Œå½¢æˆä¸€ä¸ªæ ‘çŠ¶ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼š\n",
      "\n",
      "1. å°†é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªæ€ç»´æ­¥éª¤ï¼›\n",
      "2. åœ¨æ¯ä¸€æ­¥ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„â€œæ€ç»´â€ï¼ˆå³ä¸­é—´æ¨ç†è·¯å¾„ï¼‰ï¼Œæ„å»ºå‡ºä¸€æ£µæ€ç»´æ ‘ï¼›\n",
      "3. ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰æˆ–æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰è¿›è¡Œæœç´¢ï¼›\n",
      "4. æ¯ä¸ªçŠ¶æ€é€šè¿‡åˆ†ç±»å™¨ï¼ˆç”±æç¤ºå®ç°ï¼‰æˆ–å¤šæ•°æŠ•ç¥¨è¿›è¡Œè¯„ä¼°ï¼Œä»¥å†³å®šæœ€ä½³è·¯å¾„ã€‚\n",
      "\n",
      "è¿™ç§æ–¹æ³•å¢å¼ºäº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šä¸ªæ½œåœ¨è§£å†³æ–¹æ¡ˆä¸­è¿›è¡Œè§„åˆ’å’Œé€‰æ‹©ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦è¯•é”™å’Œå‰ç»è§„åˆ’çš„å¤æ‚ä»»åŠ¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "# æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé€šå¸¸éœ€è¦è¿­ä»£æ£€ç´¢æ­¥éª¤æ‰èƒ½å›ç­”çš„é—®é¢˜\n",
    "\n",
    "query = (\n",
    "    \"ä»»åŠ¡åˆ†è§£çš„æ ‡å‡†æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\\n\\n\"\n",
    "\"ä¸€æ—¦å¾—å‡ºç­”æ¡ˆï¼Œè¯·æŸ¥æ‰¾è¯¥æ–¹æ³•çš„å¸¸è§æ‰©å±•å½¢å¼ã€‚\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d71642",
   "metadata": {},
   "source": [
    "## RAGé“¾\n",
    "åœ¨ä¸Šè¿° ä»£ç† RAG å…¬å¼ä¸­ï¼Œæˆ‘ä»¬å…è®¸ LLM é…Œæƒ…ç”Ÿæˆ å·¥å…·è°ƒç”¨ æ¥å¸®åŠ©å›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›æƒè¡¡ï¼š\n",
    "âœ… ä¼˜ç‚¹\tâš ï¸ ç¼ºç‚¹\n",
    "ä»…åœ¨éœ€è¦æ—¶æœç´¢â€”â€”LLM å¯ä»¥å¤„ç†é—®å€™è¯­ã€åç»­é—®é¢˜å’Œç®€å•æŸ¥è¯¢ï¼Œè€Œä¸ä¼šè§¦å‘ä¸å¿…è¦çš„æœç´¢ã€‚\tä¸¤æ¬¡æ¨ç†è°ƒç”¨â€”â€”æ‰§è¡Œæœç´¢æ—¶ï¼Œéœ€è¦ä¸€æ¬¡è°ƒç”¨æ¥ç”ŸæˆæŸ¥è¯¢ï¼Œå¦ä¸€æ¬¡è°ƒç”¨æ¥ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚\n",
    "ä¸Šä¸‹æ–‡æœç´¢æŸ¥è¯¢â€”â€”é€šè¿‡å°†æœç´¢è§†ä¸ºå…·æœ‰ query è¾“å…¥çš„å·¥å…·ï¼ŒLLM ä¼šåˆ›å»ºåŒ…å«å¯¹è¯ä¸Šä¸‹æ–‡çš„è‡ªå·±çš„æŸ¥è¯¢ã€‚\tæ§åˆ¶å‡å°‘â€”â€”LLM å¯èƒ½ä¼šåœ¨å®é™…éœ€è¦æ—¶è·³è¿‡æœç´¢ï¼Œæˆ–è€…åœ¨ä¸å¿…è¦æ—¶å‘å‡ºé¢å¤–æœç´¢ã€‚\n",
    "å…è®¸å¤šæ¬¡æœç´¢â€”â€”LLM å¯ä»¥æ‰§è¡Œå¤šæ¬¡æœç´¢ä»¥æ”¯æŒå•ä¸ªç”¨æˆ·æŸ¥è¯¢ã€‚\t\n",
    "\n",
    "\n",
    "å¦ä¸€ç§å¸¸è§æ–¹æ³•æ˜¯ä¸¤æ­¥é“¾ï¼Œæˆ‘ä»¬æ€»æ˜¯è¿è¡Œä¸€æ¬¡æœç´¢ï¼ˆå¯èƒ½ä½¿ç”¨åŸå§‹ç”¨æˆ·æŸ¥è¯¢ï¼‰å¹¶å°†ç»“æœä½œä¸ºå•ä¸ª LLM æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ã€‚è¿™å¯¼è‡´æ¯ä¸ªæŸ¥è¯¢åªè¿›è¡Œä¸€æ¬¡æ¨ç†è°ƒç”¨ï¼Œä»¥ç‰ºç‰²çµæ´»æ€§ä¸ºä»£ä»·é™ä½äº†å»¶è¿Ÿã€‚\n",
    "åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä¸å†å¾ªç¯è°ƒç”¨æ¨¡å‹ï¼Œè€Œæ˜¯è¿›è¡Œä¸€æ¬¡é€šè¿‡ã€‚\n",
    "æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»ä»£ç†ä¸­åˆ é™¤å·¥å…·å¹¶å°†æ£€ç´¢æ­¥éª¤åˆå¹¶åˆ°è‡ªå®šä¹‰æç¤ºä¸­æ¥å®æ–½æ­¤é“¾ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca333057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d22e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks or steps. This approach helps in understanding and solving complicated problems by focusing on one part at a time. In the context of AI and large language models (LLMs), task decomposition enables better planning and reasoning by allowing the model to \"think step by step.\"\n",
      "\n",
      "There are several ways to perform task decomposition:\n",
      "\n",
      "1. **Chain of Thought (CoT)**: The model is prompted to reason through a problem step by step (e.g., \"Let's think step by step\"), effectively decomposing the task into a linear sequence of simpler reasoning steps.\n",
      "\n",
      "2. **Tree of Thoughts (ToT)**: An extension of CoT where, at each step, multiple possible thoughts (or reasoning paths) are generated, forming a tree structure. The model explores different branches using strategies like breadth-first search (BFS) or depth-first search (DFS), evaluating which path is most promising.\n",
      "\n",
      "3. **Prompt-based Decomposition**: Using simple prompts such as \"What are the steps for XYZ?\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM in identifying subtasks.\n",
      "\n",
      "4. **Task-specific Instructions**: Applying domain-specific guidance, like asking for a \"story outline\" when writing a novel, to structure the decomposition.\n",
      "\n",
      "5. **Human-in-the-loop**: Incorporating human input to define or refine the subtasks.\n",
      "\n",
      "6. **LLM+P (LLM + Planner)**: Leveraging external classical planners that use formal languages like PDDL (Planning Domain Definition Language) to generate detailed plans. The LLM translates the problem into PDDL, invokes a planner, and then translates the resulting plan back into natural language.\n",
      "\n",
      "Task decomposition is essential for enabling AI systems to handle long-horizon, multi-step tasksâ€”such as planning, decision-making, or coordinating toolsâ€”by structuring the problem logically and sequentially.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks or steps. This approach helps in understanding and solving complicated problems by focusing on one part at a time. In the context of AI and large language models (LLMs), task decomposition enables better planning and reasoning by allowing the model to \"think step by step.\"\n",
      "\n",
      "There are several ways to perform task decomposition:\n",
      "\n",
      "1. **Chain of Thought (CoT)**: The model is prompted to reason through a problem step by step (e.g., \"Let's think step by step\"), effectively decomposing the task into a linear sequence of simpler reasoning steps.\n",
      "\n",
      "2. **Tree of Thoughts (ToT)**: An extension of CoT where, at each step, multiple possible thoughts (or reasoning paths) are generated, forming a tree structure. The model explores different branches using strategies like breadth-first search (BFS) or depth-first search (DFS), evaluating which path is most promising.\n",
      "\n",
      "3. **Prompt-based Decomposition**: Using simple prompts such as \"What are the steps for XYZ?\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM in identifying subtasks.\n",
      "\n",
      "4. **Task-specific Instructions**: Applying domain-specific guidance, like asking for a \"story outline\" when writing a novel, to structure the decomposition.\n",
      "\n",
      "5. **Human-in-the-loop**: Incorporating human input to define or refine the subtasks.\n",
      "\n",
      "6. **LLM+P (LLM + Planner)**: Leveraging external classical planners that use formal languages like PDDL (Planning Domain Definition Language) to generate detailed plans. The LLM translates the problem into PDDL, invokes a planner, and then translates the resulting plan back into natural language.\n",
      "\n",
      "Task decomposition is essential for enabling AI systems to handle long-horizon, multi-step tasksâ€”such as planning, decision-making, or coordinating toolsâ€”by structuring the problem logically and sequentially.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3a53b",
   "metadata": {},
   "source": [
    "è¿™æ˜¯ä¸€ç§å¿«é€Ÿä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‚ç”¨äºåœ¨å—é™ç¯å¢ƒä¸­è¿›è¡Œç®€å•æŸ¥è¯¢ï¼Œè€Œæˆ‘ä»¬é€šå¸¸éœ€è¦é€šè¿‡è¯­ä¹‰æœç´¢æ¥è·å–æ›´å¤šä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "ä»¥ä¸Šå†…å®¹RAGé“¾å°†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡é›†æˆåˆ°è¯¥æ¬¡è¿è¡Œçš„å•ä¸€ç³»ç»Ÿæ¶ˆæ¯ä¸­ã€‚\n",
    "æ¯”å¦‚è¯´èƒ½å¹²æ€§RAGåœ¨è¡¨è¿°é˜¶æ®µï¼Œæˆ‘ä»¬æœ‰æ—¶å¸Œæœ›åœ¨åº”ç”¨çŠ¶æ€ä¸­åŒ…å«åŸå§‹æºæ–‡æ¡£ï¼Œä»¥ä¾¿è®¿é—®æ–‡æ¡£å…ƒæ•°æ®ã€‚å¯¹äºä¸¤æ­¥é“¾çš„æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ï¼š\n",
    "å‘çŠ¶æ€æ·»åŠ ä¸€ä¸ªé”®ä»¥å­˜å‚¨æ£€ç´¢åˆ°çš„æ–‡æ¡£\n",
    "é€šè¿‡ æ·»åŠ æ–°èŠ‚ç‚¹æ¨¡å‹å‰é’©å­ç”¨æ¥å¡«å……é‚£ä¸ªå¯†é’¥ï¼ˆåŒæ—¶æ³¨å…¥ä¸Šä¸‹æ–‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1eb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. SSLError(MaxRetryError(\"HTTPSConnectionPool(host='api.smith.langchain.com', port=443): Max retries exceeded with url: /runs/multipart (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)')))\"))\n",
      "Content-Length: 3784\n",
      "API Key: lsv2_********************************************1atrace=019bbbf6-b5e2-76b3-9b4b-57e0dd71239d,id=019bbbf6-b5e2-76b3-9b4b-57e0dd71239d; trace=019bbbf6-b5e2-76b3-9b4b-57e0dd71239d,id=019bbbf6-b5ef-7361-81c4-3a1b86be3fda; trace=019bbbf6-b5e2-76b3-9b4b-57e0dd71239d,id=019bbbf6-b5ef-7361-81c4-3a1b86be3fda; trace=019bbbf6-b5e2-76b3-9b4b-57e0dd71239d,id=019bbbf6-b7db-7623-89ca-405897770df7; trace=019bbbf6-b5e2-76b3-9b4b-57e0dd71239d,id=019bbbf6-b7dc-77c0-8099-b75bfcb9bd07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "\n",
      "Use the following context to answer the query:\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "\n",
      "Use the following context to answer the query:\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Task decomposition** is the process of breaking down a complex task into smaller, more manageable subtasks or steps. This allows for better planning and execution, particularly in artificial intelligence and agent systems, by enabling a clearer understanding of the necessary actions to achieve a goal.\n",
      "\n",
      "According to the provided context:\n",
      "\n",
      "- Task decomposition is a key component of **planning**, where an agent identifies the individual steps involved in completing a complicated task.\n",
      "- It is supported by techniques like **Chain of Thought (CoT)**, which prompts models to â€œthink step by step,â€ decomposing difficult problems into simpler reasoning steps.\n",
      "- An extension called **Tree of Thoughts (ToT)** explores multiple reasoning paths at each step, forming a tree-like structure of possible solutions using search strategies such as breadth-first search (BFS) or depth-first search (DFS).\n",
      "- Decomposition can be achieved through:\n",
      "  1. **LLM prompting**, e.g., asking \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\n",
      "  2. **Task-specific instructions**, such as requesting a story outline when writing a novel.\n",
      "  3. **Human input** to guide the breakdown.\n",
      "- Another method, **LLM+P**, uses an external classical planner with **PDDL (Planning Domain Definition Language)** to formally model and solve the planning problem, outsourcing the decomposition to specialized tools.\n",
      "- In systems like **HuggingGPT**, task decomposition is performed by an LLM that parses user requests into structured tasks with attributes like type, ID, dependencies, and argumentsâ€”enabling coordinated use of multiple expert models.\n",
      "\n",
      "In summary, task decomposition enhances problem-solving efficiency by structuring complex objectives into ordered, executable stepsâ€”either internally via prompting and reasoning methods (like CoT or ToT), or externally via formal planners or human guidance.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Task decomposition** is the process of breaking down a complex task into smaller, more manageable subtasks or steps. This allows for better planning and execution, particularly in artificial intelligence and agent systems, by enabling a clearer understanding of the necessary actions to achieve a goal.\n",
      "\n",
      "According to the provided context:\n",
      "\n",
      "- Task decomposition is a key component of **planning**, where an agent identifies the individual steps involved in completing a complicated task.\n",
      "- It is supported by techniques like **Chain of Thought (CoT)**, which prompts models to â€œthink step by step,â€ decomposing difficult problems into simpler reasoning steps.\n",
      "- An extension called **Tree of Thoughts (ToT)** explores multiple reasoning paths at each step, forming a tree-like structure of possible solutions using search strategies such as breadth-first search (BFS) or depth-first search (DFS).\n",
      "- Decomposition can be achieved through:\n",
      "  1. **LLM prompting**, e.g., asking \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\n",
      "  2. **Task-specific instructions**, such as requesting a story outline when writing a novel.\n",
      "  3. **Human input** to guide the breakdown.\n",
      "- Another method, **LLM+P**, uses an external classical planner with **PDDL (Planning Domain Definition Language)** to formally model and solve the planning problem, outsourcing the decomposition to specialized tools.\n",
      "- In systems like **HuggingGPT**, task decomposition is performed by an LLM that parses user requests into structured tasks with attributes like type, ID, dependencies, and argumentsâ€”enabling coordinated use of multiple expert models.\n",
      "\n",
      "In summary, task decomposition enhances problem-solving efficiency by structuring complex objectives into ordered, executable stepsâ€”either internally via prompting and reasoning methods (like CoT or ToT), or externally via formal planners or human guidance.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
    "\n",
    "# ç»§æ‰¿ AgentStateï¼ˆé€šå¸¸åŒ…å« messages ç­‰å­—æ®µï¼‰ï¼Œå¹¶æ‰©å±•ä¸€ä¸ªæ–°å­—æ®µ contextï¼Œç”¨äºå­˜å‚¨æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ã€‚\n",
    "# è¿™æ ·æ•´ä¸ª Agent çš„çŠ¶æ€ä¸ä»…åŒ…å«å¯¹è¯æ¶ˆæ¯ï¼Œè¿˜èƒ½æºå¸¦å¤–éƒ¨æ£€ç´¢çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "class State(AgentState):\n",
    "    context: list[Document]\n",
    "\n",
    "# ç”¨æˆ·æé—® â†’ è‡ªåŠ¨ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ å°†æ–‡æ¡£æ‹¼æ¥åˆ°é—®é¢˜ä¸­ â†’ é€å…¥ LLM ç”Ÿæˆç­”æ¡ˆã€‚\n",
    "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
    "    state_schema = State # ä½œç”¨æ˜¯åœ¨ Agent è°ƒç”¨æ¨¡å‹å‰/åæ’å…¥è‡ªå®šä¹‰é€»è¾‘ã€‚\n",
    "\n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        last_message = state[\"messages\"][-1] # è·å–ç”¨æˆ·çš„æœ€æ–°çš„æé—®\n",
    "        # è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢ï¼Œè¿”å›ä¸é—®é¢˜æœ€ç›¸å…³çš„è‹¥å¹² Document\n",
    "        retrieved_docs = vector_store.similarity_search(last_message.text)\n",
    "        # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨ç©ºè¡Œåˆ†éš”ã€‚\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs) \n",
    "        # é€ å¢å¼ºåçš„æç¤ºï¼ˆpromptï¼‰ï¼šåŸå§‹é—®é¢˜ + æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶é™„ä¸ŠæŒ‡ä»¤â€œUse the following context...â€ã€‚\n",
    "        augmented_message_content = (\n",
    "            f\"{last_message.text}\\n\\n\"\n",
    "            \"Use the following context to answer the query:\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "        '''\n",
    "        è¿”å›è¦æ›´æ–°çš„çŠ¶æ€ï¼š\n",
    "        æ›¿æ¢æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹ä¸ºå¢å¼ºåçš„ç‰ˆæœ¬ï¼ˆæ³¨æ„ï¼šè¿™é‡Œåªä¿ç•™äº†æœ€åä¸€æ¡æ¶ˆæ¯ï¼Œå¯èƒ½ä¸¢å¤±å†å²ï¼ï¼‰ã€‚\n",
    "        ä¿å­˜ context åˆ°çŠ¶æ€ä¸­ï¼Œä¾›åç»­æ­¥éª¤ï¼ˆå¦‚æ—¥å¿—ã€è°ƒè¯•ã€å·¥å…·è°ƒç”¨ï¼‰ä½¿ç”¨ã€‚\n",
    "        '''\n",
    "        return {\n",
    "            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n",
    "            \"context\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[],\n",
    "    middleware=[RetrieveDocumentsMiddleware()],\n",
    ")\n",
    "\n",
    "# è°ƒç”¨agent\n",
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f847a",
   "metadata": {},
   "source": [
    "# ä¸‹ä¸€æ­¥\n",
    "ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†ä¸€ä¸ªç®€å•çš„ RAG åº”ç”¨ï¼Œé€šè¿‡create_agentæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åŠ å…¥æ–°åŠŸèƒ½å¹¶æ·±å…¥æ¢è®¨ï¼š\n",
    "æµä»¤ç‰ŒåŠå…¶ä»–å“åº”å¼ç”¨æˆ·ä½“éªŒä¿¡æ¯\n",
    "æ·»åŠ ä¼šè¯è®°å¿†æ”¯æŒå¤šå›åˆäº¤äº’\n",
    "æ·»åŠ é•¿æœŸè®°å¿†æ”¯æŒä¼šè¯çº¿ç¨‹é—´çš„è®°å¿†\n",
    "æ·»åŠ ç»“æ„åŒ–å“åº”\n",
    "éƒ¨ç½²ä½ çš„åº”ç”¨LangSmith éƒ¨ç½²"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
